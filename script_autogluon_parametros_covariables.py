# -*- coding: utf-8 -*-
"""script_autogluon_parametros_covariables.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uJiFfmz6PZjFZJ9oSB9p9nsQYzu6bpyv

AutoGluon - Predicción de ventas (tn) por producto para febrero 2020
"""

# 📦 1. Importar librerías
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

# 💬 Instalar AutoGluon si es necesario
!pip install autogluon.timeseries

from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame

# 📄 2. Cargar datasets
from pathlib import Path
# Rutas locales
#ruta_base = Path("D:/Repos/MCD_labo3")
ruta_base = Path("/content/drive/MyDrive/Colab Notebooks")

path_sell = ruta_base / "sell-in.txt"
path_prod = ruta_base / "tb_productos.txt"
path_stocks = ruta_base / "tb_stocks.txt"
path_encuestas = ruta_base / "encuesta_ventas.csv"
df_sellin = pd.read_csv(path_sell, sep="\t")
df_productos = pd.read_csv(path_prod, sep="\t")

# 📄 Leer lista de productos a predecir

path_id_apredecir = ruta_base / "product_id_apredecir201912.txt"
with open(path_id_apredecir, "r") as f:
    product_ids = [int(line.strip()) for line in f if line.strip().isdigit()]

# 🧹 3. Preprocesamiento
# Convertir periodo a datetime
df_sellin['timestamp'] = pd.to_datetime(df_sellin['periodo'], format='%Y%m')

# Filtrar hasta dic 2019 y productos requeridos
df_filtered = df_sellin[
    (df_sellin['timestamp'] <= '2019-12-01') &
    (df_sellin['product_id'].isin(product_ids))
]

# Agregar tn por periodo, cliente y producto
df_grouped = df_filtered.groupby(['timestamp', 'customer_id', 'product_id'], as_index=False)['tn'].sum()

df_raw = pd.read_csv(path_encuestas, sep=";")
# convertir millones_pesos a numérico
df_raw['millones_pesos'] = pd.to_numeric(df_raw['millones_pesos'], errors='coerce')
df_raw.head()

# mapear mes a número
mes_map = {
    "Enero": 1, "Febrero": 2, "Marzo": 3, "Abril": 4, "Mayo": 5, "Junio": 6,
    "Julio": 7, "Agosto": 8, "Septiembre": 9, "Octubre": 10, "Noviembre": 11, "Diciembre": 12
}
df_raw["mes_n"] = df_raw["mes"].map(mes_map)

# crear columna 'periodo' (int: YYYYMM)
df_macro = (
    df_raw
      .assign(periodo=lambda d: d["anio"]*100 + d["mes_n"])
      .loc[:, ["periodo", "millones_pesos"]]
      .rename(columns={"millones_pesos": "ventas_macro"})
)

# Convertir periodo a datetime format YYYY-MM-01
df_macro['periodo'] = pd.to_datetime(df_macro['periodo'], format='%Y%m')

#renombrar columnas
df_macro.rename(columns={'periodo': 'timestamp'}, inplace=True)

df_macro.head()
df_macro.info()

# Agregar tn total por periodo y producto
df_monthly_product = df_grouped.groupby(['timestamp', 'product_id'], as_index=False)['tn'].sum()
df_monthly_product['item_id'] = df_monthly_product['product_id']

# Unir info de producto (covariables estáticas)
df_monthly_product = df_monthly_product.merge(
    df_productos[['product_id', 'cat1', 'cat2', 'brand', 'sku_size']],
    on='product_id', how='left'
)


df_monthly_product['year'] = df_monthly_product['timestamp'].dt.year
df_monthly_product['month'] = df_monthly_product['timestamp'].dt.month
df_monthly_product['quarter'] = df_monthly_product['timestamp'].dt.quarter
df_monthly_product['is_year_start'] = df_monthly_product['timestamp'].dt.is_year_start.astype(int)
df_monthly_product['is_year_end'] = df_monthly_product['timestamp'].dt.is_year_end.astype(int)

# --- NUEVO: Rolling mean y lags ---
df_monthly_product = df_monthly_product.sort_values(['product_id', 'timestamp'])

# Número de meses desde el inicio de la serie para cada producto
df_monthly_product['months_since_start'] = (
    df_monthly_product.groupby('product_id').cumcount()
)

# Ventas acumuladas hasta el mes anterior
df_monthly_product['tn_cumsum'] = (
    df_monthly_product.groupby('product_id')['tn'].cumsum().shift(1).fillna(0)
)

# Rolling mean de los últimos 3 meses (sin incluir el mes actual)
df_monthly_product['tn_roll3'] = (
    df_monthly_product.groupby('product_id')['tn']
    .transform(lambda x: x.shift(1).rolling(window=3, min_periods=1).mean())
)

# Desviación estándar de los últimos 3 meses (sin incluir el mes actual)
df_monthly_product['tn_std3'] = (
    df_monthly_product.groupby('product_id')['tn']
    .transform(lambda x: x.shift(1).rolling(window=3, min_periods=1).std())
)


# Baseline: promedio de los últimos 3 meses (sin incluir el mes actual)
df_monthly_product['baseline_3m'] = (
    df_monthly_product.groupby('product_id')['tn']
    .transform(lambda x: x.shift(1).rolling(window=3, min_periods=1).mean())
)



# Lag de 1 mes
df_monthly_product['tn_lag1'] = (
    df_monthly_product.groupby('product_id')['tn']
    .shift(1)
)

# Lag de 2 meses
df_monthly_product['tn_lag2'] = (
    df_monthly_product.groupby('product_id')['tn']
    .shift(2)
)

df_monthly_product.head()

df = df_monthly_product.merge(df_macro, on="timestamp", how="left")

# 👉 Si faltan meses, rellena con forward-fill o con la media móvil:
df["ventas_macro"] = df["ventas_macro"].fillna(method="ffill")

# 🎁 Features extra (opcional)
for lag in [1, 3, 6, 12]:
    df[f"ventas_macro_lag{lag}"] = df["ventas_macro"].shift(lag)

# también ratios de crecimiento
#df["macro_pct_change"] = df["ventas_macro"].pct_change()

df.head()
df.info()

df_monthly_product.head()

# Agregar columna 'item_id' para AutoGluon
# (Ya agregada en el bloque anterior)

import numpy as np
# Reemplaza inf y -inf por NaN
df.replace([np.inf, -np.inf], np.nan, inplace=True)

# Muestra cuántos NaN quedan por columna
print("Valores NaN por columna antes de limpiar:")
print(df.isna().sum())

# Opciones para limpiar:
# 1. Rellenar NaN numéricos con 0 (o puedes usar .fillna(method="ffill") si prefieres)
df = df.fillna(0)

# 2. Si prefieres eliminar filas con NaN (menos recomendado para series de tiempo):
# df = df.dropna()

# Verifica que ya no haya NaN ni inf
print("Valores NaN después de limpiar:", df.isna().sum().sum())
# Solo aplicar isinf a columnas numéricas
numeric_df = df.select_dtypes(include=[np.number])
print("Valores inf después de limpiar:", np.isinf(numeric_df.to_numpy()).sum())

import numpy as np

# 1. Reemplaza inf y -inf por NaN
df.replace([np.inf, -np.inf], np.nan, inplace=True)

# 2. Muestra cuántos NaN quedan por columna
print("Valores NaN por columna antes de limpiar:")
print(df.isna().sum())

# 3. Elimina filas con NaN en columnas críticas (item_id, timestamp, tn)
df = df.dropna(subset=['item_id', 'timestamp', 'tn'])

# 4. Rellena el resto de NaN con 0
df = df.fillna(0)

# 5. Verifica que no haya NaN ni inf
print("Valores NaN después de limpiar:", df.isna().sum().sum())
numeric_df = df.select_dtypes(include=[np.number])
print("Valores inf después de limpiar:", np.isinf(numeric_df.to_numpy()).sum())

# 6. Verifica tipos
print(df.dtypes)

# ⏰ 4. Crear TimeSeriesDataFrame
ts_data = TimeSeriesDataFrame.from_data_frame(
    df,
    id_column='item_id',
    timestamp_column='timestamp'
)

# Completar valores faltantes
ts_data = ts_data.fill_missing_values()

#drop columnas cat1, cat2, brand
ts_data = ts_data.drop(columns=['cat1', 'cat2', 'brand'])

ts_data.info()

#mostrar(ts_data.head())
ts_data.head()



# ⚙️ 5. Definir y entrenar predictor
predictor = TimeSeriesPredictor(
    prediction_length=2,
    target='tn',
    freq='MS',  # Frecuencia mensual (Month Start)
    eval_metric ="MAPE" ,  # Métrica de evaluación


)



predictor.fit(
    ts_data,
    num_val_windows=2,
    presets="best_quality",
    refit_full=True,

)

predictor.leaderboard(silent=True)

# 🔮 6. Generar predicción
forecast = predictor.predict(ts_data)

from pprint import pprint
info = predictor.get_model_info('WeightedEnsemble')
pprint(info['info']['weights'])  # pesos por modelo base

# Extraer predicción media y filtrar febrero 2020
forecast_mean = forecast['mean'].reset_index()
print(forecast_mean.columns)

# Tomar solo item_id y la predicción 'mean'
resultado = forecast['mean'].reset_index()[['item_id', 'mean']]
resultado.columns = ['product_id', 'tn']

# Filtrar solo febrero 2020
resultado = forecast['mean'].reset_index()
resultado = resultado[resultado['timestamp'] == '2020-02-01']

# Renombrar columnas
resultado = resultado[['item_id', 'mean']]
resultado.columns = ['product_id', 'tn']


# Extraer baseline para febrero 2020
baseline_feb = df_monthly_product[df_monthly_product['timestamp'] == '2020-02-01'][['product_id', 'baseline_3m']]
baseline_feb['baseline_3m'] = baseline_feb['baseline_3m'].fillna(0)  # <-- primero rellenar NaN

# Asegura que ambos sean int o str, pero iguales
resultado['product_id'] = resultado['product_id'].astype(int)
baseline_feb['product_id'] = baseline_feb['product_id'].astype(int)

print("IDs en resultado:", set(resultado['product_id']))
print("IDs en baseline_feb:", set(baseline_feb['product_id']))
print("IDs en común:", set(resultado['product_id']).intersection(set(baseline_feb['product_id'])))

# Ahora sí, merge
resultado2 = resultado.merge(baseline_feb, on='product_id', how='left')

# Blending
resultado2['tn_blend'] = 1 * resultado2['tn'] + 0 * resultado2['baseline_3m']
resultado2['tn'] = resultado2['tn_blend']

resultado2 = resultado.merge(baseline_feb, on='product_id', how='left')
resultado2['baseline_3m'] = resultado2['baseline_3m'].fillna(0)
resultado2['tn_blend'] = 1 * resultado2['tn'] + 0 * resultado2['baseline_3m']
resultado2['tn'] = resultado2['tn_blend']

print(resultado2.isna().sum())
print(resultado2.head())

# dropear las columnas baseline y tn_blend
resultado2 = resultado2.drop(columns=['baseline_3m', 'tn_blend'])

# 💾 7. Guardar archivo
# Crear carpeta de salida si no existe
output_dir = ruta_base / "salidas"
output_dir.mkdir(exist_ok=True)
path_out = output_dir / "predicciones_autogluon_16.csv"
resultado2.to_csv(path_out, index=False)
resultado2.head()

